{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOwX68XdhBkJLOEjZ3OgTD1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\n","%ls\n","%cd akbank-genai-chatbot/\n","%ls\n","!mv /content/GeminiAPI.ipynb .\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tTGKqxXLq40H","executionInfo":{"status":"ok","timestamp":1760880923637,"user_tz":-180,"elapsed":451,"user":{"displayName":"Gizem","userId":"13746983046960866260"}},"outputId":"39524d0a-7068-4935-9fbe-74284f358af2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mdata\u001b[0m/  README.md\n","[Errno 2] No such file or directory: 'akbank-genai-chatbot/'\n","/content/akbank-genai-chatbot\n","\u001b[0m\u001b[01;34mdata\u001b[0m/  README.md\n","mv: cannot stat '/content/GeminiAPI.ipynb': No such file or directory\n","\u001b[0m\u001b[01;34mdata\u001b[0m/  README.md\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzFf8x-pik_1","executionInfo":{"status":"ok","timestamp":1760880013385,"user_tz":-180,"elapsed":622,"user":{"displayName":"Gizem","userId":"13746983046960866260"}},"outputId":"215a0b4f-1b69-4626-dc79-69a95489e806"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'akbank-genai-chatbot'...\n","remote: Enumerating objects: 15, done.\u001b[K\n","remote: Counting objects: 100% (15/15), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","Receiving objects: 100% (15/15), 10.34 KiB | 10.34 MiB/s, done.\n","Resolving deltas: 100% (1/1), done.\n","remote: Total 15 (delta 1), reused 10 (delta 0), pack-reused 0 (from 0)\u001b[K\n"]}],"source":["!git clone https://github.com/gizemnuryilmaz/akbank-genai-chatbot.git"]},{"cell_type":"code","source":["!pip install langchain_google_genai --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEAgdviUm6Tl","executionInfo":{"status":"ok","timestamp":1760880037550,"user_tz":-180,"elapsed":24162,"user":{"displayName":"Gizem","userId":"13746983046960866260"}},"outputId":"33b535e1-523f-4746-cc53-e126c7276709"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.0 which is incompatible.\n","google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","from google.colab import userdata\n","import os\n","os.environ[\"GOOGLE_API_KEY\"]=userdata.get(\"GOOGLE_API_KEY\")\n","gemini=ChatGoogleGenerativeAI(\n","    model=\"gemini-2.0-flash\"\n",")\n","#response=gemini.invoke(\"Kahve nedir?\")\n","#print(response.content)\n"],"metadata":{"id":"XgHUCsiIi0jy","executionInfo":{"status":"ok","timestamp":1760880110734,"user_tz":-180,"elapsed":944,"user":{"displayName":"Gizem","userId":"13746983046960866260"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"66TgzdIWlFD6"}},{"cell_type":"code","source":["!pip install -qU \"langchain[google-genai]\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXXgwW33nvuT","executionInfo":{"status":"ok","timestamp":1760880081673,"user_tz":-180,"elapsed":20831,"user":{"displayName":"Gizem","userId":"13746983046960866260"}},"outputId":"09898dc9-a5fa-4d82-eff0-896d13d3b863"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/155.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import os\n","from google import genai\n","from google.genai import types\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","# 1. API Anahtarını Ayarlama\n","# Ortam değişkeninden anahtarı alacaktır.\n","try:\n","    client = genai.Client()\n","except Exception as e:\n","    print(f\"Hata: Gemini API anahtarı ayarlanmamış veya geçersiz. Lütfen 'export GEMINI_API_KEY=\\\"...\\\"' komutunu kullanın. Detay: {e}\")\n","    exit()\n","\n","# 2. Veri Seti (Knowledge Base) Hazırlama\n","# Bu basit örnekte, veri setimiz bir Python listesi içindeki metin parçalarıdır.\n","KNOWLEDGE_BASE = [\n","    \"Gemini 2.0 Flash, Google tarafından geliştirilmiş, hızlı ve çok modlu bir büyük dil modelidir (LLM).\",\n","    \"RAG (Retrieval-Augmented Generation), harici bir bilgi tabanından alakalı verileri alıp, bu verileri LLM'e bağlam olarak sağlayarak yanıt üretimini geliştiren bir tekniktir.\",\n","    \"Gemini modelleri, hem metin hem de görsel/işitsel verileri işleyebilir ve yanıtlayabilir.\",\n","    \"Gemini 2.0 Flash'ın temel avantajları arasında düşük gecikme süresi, yüksek hız ve multimodal yetenekler bulunur.\",\n","    \"Bir RAG sisteminde, veriler genellikle 'embedding' adı verilen sayısal vektörlere dönüştürülür.\",\n","    \"En iyi uygulama, verileri 200 ile 500 kelimelik küçük ve bağlamsal parçalara (chunk) ayırmaktır.\",\n","]\n","\n","# Gömme (Embedding) modelini seçme\n","EMBEDDING_MODEL = 'gemini-embedding-001' # Gemini için önerilen embedding modeli\n","\n","def create_embeddings(texts):\n","    \"\"\"Metin listesi için gömme (embedding) vektörlerini oluşturur.\"\"\"\n","    print(\"Veri setinin embedding'leri oluşturuluyor...\")\n","    response = client.models.embed_content(\n","        model=EMBEDDING_MODEL,\n","        contents=texts,\n","    )\n","    embeddings_list = [item.values for item in response.embeddings]\n","    # Vektörleri numpy dizisine dönüştür\n","    return np.array(embeddings_list)\n","\n","# 3. Vektör Veritabanı Oluşturma (Bu örnekte bellek içi bir matris)\n","knowledge_base_embeddings = create_embeddings(KNOWLEDGE_BASE)\n","print(f\"Oluşturulan embedding sayısı: {len(knowledge_base_embeddings)}\")\n","\n","def retrieve_context(query, top_k=2):\n","    \"\"\"Sorgu ile en alakalı veri parçalarını bulur (Geri Alma).\"\"\"\n","    # Sorgunun embedding'ini oluştur\n","    query_embedding_response = client.models.embed_content(\n","        model=EMBEDDING_MODEL,\n","        contents=[query]\n","    )\n","    query_embeddings_list = [item.values for item in query_embedding_response.embeddings]\n","    query_embedding = np.array(query_embeddings_list)\n","\n","    # Kosinüs benzerliği ile sorgu ve bilgi tabanı vektörleri arasındaki benzerliği hesapla\n","    # reshape(1, -1) sorgu vektörünün tek satırlı bir matris olmasını sağlar\n","    similarities = cosine_similarity(query_embedding.reshape(1, -1), knowledge_base_embeddings)\n","\n","    # En yüksek benzerliğe sahip parçaların indekslerini al\n","    top_indices = np.argsort(similarities[0])[::-1][:top_k]\n","\n","    # En alakalı parçaları döndür\n","    retrieved_chunks = [KNOWLEDGE_BASE[i] for i in top_indices]\n","    print(f\"\\n[Geri Alınan Bağlam ({top_k} adet)]:\")\n","    for chunk in retrieved_chunks:\n","        print(f\"- {chunk}\")\n","\n","    return \"\\n\".join(retrieved_chunks)\n","\n","def generate_response(query, context):\n","    \"\"\"Geri alınan bağlamı kullanarak Gemini'den yanıt üretir (Üretim).\"\"\"\n","\n","    # Modelden istediğimiz davranışı belirten prompt şablonu\n","    prompt = f\"\"\"\n","    Aşağıdaki 'BAĞLAM' bölümünde sağlanan bilgilere dayanarak '{query}' sorusuna yanıt verin.\n","    Eğer bağlamda yeterli bilgi yoksa, 'Üzgünüm, bu bilgiye sahip değilim.' diye cevap verin.\n","\n","    BAĞLAM:\n","    {context}\n","\n","    SORU: {query}\n","    YANIT:\n","    \"\"\"\n","\n","    print(\"\\n[Yanıt Üretiliyor...]\")\n","    response = client.models.generate_content(\n","        model='gemini-2.0-flash', # Hızlı ve verimli model\n","        contents=prompt\n","    )\n","\n","    return response.text\n","\n","def simple_rag_pipeline(query):\n","    \"\"\"Tüm RAG sürecini çalıştırır.\"\"\"\n","    # 4. Geri Alma (Retrieval)\n","    context = retrieve_context(query)\n","\n","    # 5. Üretim (Generation)\n","    final_response = generate_response(query, context)\n","\n","    print(\"\\n--- Nihai Yanıt ---\")\n","    print(final_response)\n","    print(\"--------------------\")\n","\n","# Örnek Sorgu\n","query1 = \"Gemini 2.0 Flash'ın ana faydaları nelerdir?\"\n","simple_rag_pipeline(query1)\n","\n","print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","# Bağlamda olmayan bir sorgu\n","query2 = \"Fransa'nın başkenti neresidir?\"\n","simple_rag_pipeline(query2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VzG0iudUoD5A","executionInfo":{"status":"ok","timestamp":1760880106357,"user_tz":-180,"elapsed":24659,"user":{"displayName":"Gizem","userId":"13746983046960866260"}},"outputId":"2f42965b-b0ad-4b60-9344-0c8e45206ee4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Veri setinin embedding'leri oluşturuluyor...\n","Oluşturulan embedding sayısı: 6\n","\n","[Geri Alınan Bağlam (2 adet)]:\n","- Gemini 2.0 Flash'ın temel avantajları arasında düşük gecikme süresi, yüksek hız ve multimodal yetenekler bulunur.\n","- Gemini 2.0 Flash, Google tarafından geliştirilmiş, hızlı ve çok modlu bir büyük dil modelidir (LLM).\n","\n","[Yanıt Üretiliyor...]\n","\n","--- Nihai Yanıt ---\n","Gemini 2.0 Flash'ın ana faydaları düşük gecikme süresi, yüksek hız ve multimodal yetenekleridir.\n","\n","--------------------\n","\n","==================================================\n","\n","\n","[Geri Alınan Bağlam (2 adet)]:\n","- RAG (Retrieval-Augmented Generation), harici bir bilgi tabanından alakalı verileri alıp, bu verileri LLM'e bağlam olarak sağlayarak yanıt üretimini geliştiren bir tekniktir.\n","- Gemini 2.0 Flash, Google tarafından geliştirilmiş, hızlı ve çok modlu bir büyük dil modelidir (LLM).\n","\n","[Yanıt Üretiliyor...]\n","\n","--- Nihai Yanıt ---\n","Üzgünüm, bu bilgiye sahip değilim.\n","\n","--------------------\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tGNY7E-Pputr"},"execution_count":null,"outputs":[]}]}