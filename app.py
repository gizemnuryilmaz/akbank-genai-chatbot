import streamlit as st
import os
import glob
import numpy as np
import pandas as pd # <-- YENÄ°: Pandas kÃ¼tÃ¼phanesini ekliyoruz
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
from dotenv import load_dotenv
import time # <-- Yeni import
import chromadb # <-- YENÄ° Ä°MPORT
import requests
import json

# --- RAG Ä°Ã‡Ä°N Ã–ZEL CSV Ä°ÅLEME FONKSÄ°YONU ---

def create_rag_document_from_row(row: pd.Series) -> str:
    """
    Pandas Series (tek bir CSV satÄ±rÄ±) objesini RAG iÃ§in anlamlÄ± bir string'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    
    Bu format, daha Ã¶nceki yanÄ±tlarda belirlenen sÃ¼tun baÅŸlÄ±klarÄ±na uygundur.
    """
    
    # NaN veya boÅŸ deÄŸerleri yÃ¶netmek iÃ§in yardÄ±mcÄ± fonksiyon
    def safe_get(key):
        # row.get(key, '') ile sÃ¼tun bulunamazsa veya deÄŸeri boÅŸsa/NaN ise None dÃ¶ndÃ¼r
        value = str(row.get(key, '')).strip()
        if value.lower() in ('nan', '', '-') or key not in row:
            return None
        return value

    # Veri setinizdeki sÃ¼tun baÅŸlÄ±klarÄ±nÄ± kullanarak deÄŸerleri al
    name = safe_get('name')
    roaster = safe_get('roaster')
    roast = safe_get('roast')
    rating = safe_get('rating')
    origin_1 = safe_get('origin_1')
    origin_2 = safe_get('origin_2')
    price = safe_get('100g_USD')
    desc_1 = safe_get('desc_1')
    desc_2 = safe_get('desc_2')
    desc_3 = safe_get('desc_3')
    review_date = safe_get('review_date')

    parts = []

    # 1. TanÄ±mlayÄ±cÄ± Bilgiler
    if name and roaster:
        parts.append(f"Kahve AdÄ±: {name} (Kavurucu: {roaster}).")
    
    # 2. KÃ¶kenler
    origins = [o for o in [origin_1, origin_2] if o]
    if origins:
        parts.append(f"KÃ¶ken Ãœlke(ler): {', '.join(origins)}.")
    
    # 3. Kavurma, Puan ve Fiyat
    if roast:
        parts.append(f"Kavurma Seviyesi: {roast}.")
    if rating:
        parts.append(f"Puan: {rating}.")
    if price:
        parts.append(f"FiyatÄ±: {price} USD/100g.")
    
    # 4. AÃ§Ä±klamalar (TadÄ±m NotlarÄ±)
    descriptions = [d for d in [desc_1, desc_2, desc_3] if d]
    if descriptions:
        parts.append(f"TadÄ±m NotlarÄ±: {', '.join(descriptions)}.")
        
    # 5. Ekstra Bilgi
    if review_date:
        parts.append(f"Ä°nceleme Tarihi: {review_date}.")

    # TÃ¼m parÃ§alarÄ± tek bir string belge olarak birleÅŸtir
    return " ".join(parts)


# --- 1. YÃœKLEME VE KURULUM (GÃœNCELLENMÄ°Å) ---

# .env dosyasÄ±ndaki API anahtarÄ±nÄ± yÃ¼kle
load_dotenv() 
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")

try:
    # API anahtarÄ±nÄ± ortam deÄŸiÅŸkeninden alarak ayarla
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        st.error("Hata: GOOGLE_API_KEY bulunamadÄ± veya boÅŸ. LÃ¼tfen .env dosyanÄ±zÄ± kontrol edin.")
        st.stop()
    genai.configure(api_key=api_key)
except Exception as e:
    st.error(f"API anahtarÄ± ayarlanÄ±rken bir hata oluÅŸtu: {e}")
    st.stop()

# Embedding modeli
EMBEDDING_MODEL = 'gemini-embedding-001'



BATCH_SIZE = 4000  # Kota aÅŸÄ±mÄ±nÄ± yÃ¶netmek iÃ§in
SLEEP_TIME = 1  # saniye (Kota aÅŸÄ±mÄ±nÄ± yÃ¶netmek iÃ§in)
CHROMA_PATH = "./chroma_db" # <-- VektÃ¶r DB'nin kaydedileceÄŸi klasÃ¶r
LM_STUDIO_API_URL = "http://localhost:1234/v1/embeddings"
LOCAL_EMBEDDING_MODEL = "text-embedding-nomic-embed-text-v1.5" # BaÅŸarÄ±yla test ettiÄŸiniz model adÄ±

@st.cache_resource
def load_and_embed_knowledge_base():
    """
    1. ChromaDB'yi kontrol eder. Veri varsa, API Ã§aÄŸrÄ±sÄ± yapmadan yÃ¼kler.
    2. Veri yoksa CSV'leri okur, API ile embedding oluÅŸturur ve ChromaDB'ye kaydeder.
    """
    
    # 1. ChromaDB Ä°stemcisini BaÅŸlat ve Koleksiyonu Al/OluÅŸtur
    os.makedirs(CHROMA_PATH, exist_ok=True)
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    collection_name = "coffee_rag_data"
    collection = client.get_or_create_collection(name=collection_name)
    
    # 2. **KOTA KORUMASI:** ChromaDB'de kayÄ±tlÄ± veri var mÄ± kontrol et
    if collection.count() > 0:
        count = collection.count()
        print(f"ChromaDB'den {count} adet belge yÃ¼klendi (API Ã§aÄŸrÄ±sÄ± atlandÄ±).")
        st.info(f"VektÃ¶r veritabanÄ±ndan {count} belge yÃ¼klendi. (API KotasÄ± Korundu)")
        
        # TÃ¼m verileri (metin ve vektÃ¶r) koleksiyondan Ã§ek
        all_data = collection.get(
            ids=collection.get()['ids'], 
            include=['documents', 'embeddings']
        )
        
        # RAG pipeline'Ä±nÄ±n beklediÄŸi in-memory formatÄ±na geri dÃ¶nÃ¼ÅŸtÃ¼r
        KNOWLEDGE_BASE = all_data['documents']
        knowledge_base_embeddings = np.array(all_data['embeddings']) 
        
        # API Ã§aÄŸrÄ±sÄ± yapmadan ve yeniden hesaplama yapmadan fonksiyondan Ã§Ä±k
        return KNOWLEDGE_BASE, knowledge_base_embeddings 
        
    # --- KOLEKSÄ°YON BOÅ Ä°SE: CSV OKU VE EMBEDDING OLUÅTUR (YENÄ°DEN HESAPLAMA) ---
    
    KNOWLEDGE_BASE = []
    data_files_path = "data/*.csv" 
    
    print("ChromaDB boÅŸ. CSV DosyalarÄ± okunuyor ve RAG belgelerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
    # ... (CSV okuma ve KNOWLEDGE_BASE doldurma mantÄ±ÄŸÄ± aynÄ± kalÄ±r) ...
    for file_path in glob.glob(data_files_path):
        try:
            df = pd.read_csv(file_path, sep=',', encoding='utf-8', keep_default_na=False)
            for index, row in df.iterrows():
                content = create_rag_document_from_row(row)
                if content.strip():
                    KNOWLEDGE_BASE.append(content)
        except Exception as e:
            print(f"HATA: {file_path} okunurken veya iÅŸlenirken sorun oluÅŸtu: {e}")

    if not KNOWLEDGE_BASE:
        st.error("HATA: HiÃ§ dolu CSV satÄ±rÄ± bulunamadÄ±. 'data' klasÃ¶rÃ¼ndeki CSV'yi kontrol edin.")
        return None, None

    print(f"BaÅŸarÄ±yla {len(KNOWLEDGE_BASE)} adet RAG belgesi yÃ¼klendi.")
    st.info(f"{len(KNOWLEDGE_BASE)} belge iÃ§in Yerel LM Studio API ile embedding oluÅŸturuluyor...")

    all_embeddings = []

    # KNOWLEDGE_BASE'i kÃ¼Ã§Ã¼k parÃ§alara bÃ¶l (Batching ve Rate Limiting)
    for i in range(0, len(KNOWLEDGE_BASE), BATCH_SIZE):
        batch = KNOWLEDGE_BASE[i:i + BATCH_SIZE]
        print(f"Toplu iÅŸlem {i//BATCH_SIZE + 1} baÅŸlatÄ±lÄ±yor ({len(batch)} belge)...")
        
        try:
            # 1. YEREL LM STUDIO Ä°Ã‡Ä°N GÃ–VDEYÄ° OLUÅTUR
            payload = {
                "model": LOCAL_EMBEDDING_MODEL, 
                "input": batch # LM Studio API'si genellikle liste formatÄ±nÄ± destekler
            }
            headers = {"Content-Type": "application/json"}

            # 2. requests.post Ä°LE YEREL API Ä°STEÄÄ°NÄ° GÃ–NDER
            response = requests.post(
                LM_STUDIO_API_URL, 
                headers=headers, 
                data=json.dumps(payload)
            )
            response.raise_for_status() # HTTP hatalarÄ±nÄ± yakala
            
            data = response.json()

            # 3. EMBEDDING'LERÄ° Ã‡IKAR VE TOPLA
            if data and 'data' in data:
                # Gelen yanÄ±tta birden fazla embedding bulunur (batch kadar)
                for item in data['data']:
                    all_embeddings.append(item['embedding'])
            else:
                raise Exception("Yerel API'den geÃ§erli embedding verisi alÄ±namadÄ±.")
                
            print(f"Toplu iÅŸlem {i//BATCH_SIZE + 1} tamamlandÄ±. {SLEEP_TIME} saniye bekleniyor...")
            
            # 4. Yerel sunucuda kararlÄ±lÄ±k iÃ§in bekleme
            time.sleep(SLEEP_TIME) 
            
        except Exception as e:
            # Hata oluÅŸursa Streamlit'te gÃ¶ster ve None dÃ¶ndÃ¼r
            st.error(f"Embedding oluÅŸturulurken kritik hata (Toplu Ä°ÅŸlem {i//BATCH_SIZE + 1}): {e}")
            print(f"Hata detayÄ±: {e}")
            return None, None
# 3. BAÅARILI HESAPLAMA SONRASI ChromaDB'ye Kaydet
    print("TÃ¼m embedding'ler oluÅŸturuldu. ChromaDB'ye kalÄ±cÄ± olarak kaydediliyor...")
    doc_ids = [f"doc_{i}" for i in range(len(KNOWLEDGE_BASE))]
    
    try:
        collection.add(
            embeddings=all_embeddings,
            documents=KNOWLEDGE_BASE,
            ids=doc_ids
        )
        print("Veri ChromaDB'ye baÅŸarÄ±yla kaydedildi.")
        st.success("Embedding'ler oluÅŸturuldu ve bir sonraki Ã§alÄ±ÅŸtÄ±rma iÃ§in kaydedildi.")
        
    except Exception as e:
        st.error(f"ChromaDB'ye kaydederken hata oluÅŸtu: {e}")
        # KayÄ±t hatasÄ± olsa bile bellek iÃ§i veriyi dÃ¶ndÃ¼r
        knowledge_base_embeddings = np.array(all_embeddings)
        return KNOWLEDGE_BASE, knowledge_base_embeddings

    # <-- Ä°ÅLEM TAMAMLANDIÄINDA EKLENMESÄ° GEREKEN BAÅARILI DÃ–NÃœÅ BURADA!
    # Yeni oluÅŸturulan veriyi ve embedding'i dÃ¶ndÃ¼r
    knowledge_base_embeddings = np.array(all_embeddings)
    print(f"DEBUG: YÃœKLENEN BÄ°LGÄ° TABANI VektÃ¶r Boyutu: {knowledge_base_embeddings.shape[1]}")
    return KNOWLEDGE_BASE, knowledge_base_embeddings # <-- Bu satÄ±r EKSÄ°KTÄ°!

# KaynaklarÄ± yÃ¼kle (Bu fonksiyon sadece ilk Ã§alÄ±ÅŸtÄ±rmada Ã§alÄ±ÅŸÄ±r)
KNOWLEDGE_BASE, knowledge_base_embeddings = load_and_embed_knowledge_base()

# --- 2. RAG FONKSÄ°YONLARI (DEÄÄ°ÅÄ°KLÄ°K YOK) ---
# ... (retrieve_context, generate_response, simple_rag_pipeline fonksiyonlarÄ± aynÄ± kalÄ±r)



# Global Chroma Collection objesi (load_and_embed_knowledge_base'den yÃ¼klenecek)
CHROMA_COLLECTION = None
COLLECTION_NAME = "coffee_rag_data"
HEADERS = {"Content-Type": "application/json"}


def retrieve_context(query, top_k=13):
    """
    LM Studio'dan sorgu embedding'ini alÄ±r ve ChromaDB'ye sorgu gÃ¶nderir.
    """
    global CHROMA_COLLECTION
    
    # Global koleksiyonu kontrol et
    if CHROMA_COLLECTION is None:
        # ChromaDB koleksiyonunu bir kez yÃ¼kle (EÄŸer load_and_embed_knowledge_base gÃ¼ncellendiyse)
        try:
            client = chromadb.PersistentClient(path=CHROMA_PATH)
            CHROMA_COLLECTION = client.get_collection(name=COLLECTION_NAME)
        except Exception:
             return "Hata: ChromaDB koleksiyonuna eriÅŸilemiyor. LÃ¼tfen uygulamayÄ± yeniden baÅŸlatÄ±n."


    try:
        # 1. SORGUNUN EMBEDDING'Ä°NÄ° OLUÅTUR (LM Studio ile)
        payload = {
            "model": LOCAL_EMBEDDING_MODEL,
            "input": query
        }
        
        response = requests.post(
            LM_STUDIO_API_URL, 
            headers=HEADERS, 
            data=json.dumps(payload)
        )
        response.raise_for_status() 
        
        data = response.json()
        
        if not (data and 'data' in data and data['data']):
            raise Exception("Yerel API'den geÃ§erli sorgu embedding'i alÄ±namadÄ±.")

        # Tek bir sorgu vektÃ¶rÃ¼nÃ¼ al
        query_vector = data['data'][0]['embedding']

        # 2. CHROMADB'DE EN YAKIN BELGELERÄ° ARA
        results = CHROMA_COLLECTION.query(
            query_embeddings=[query_vector], # Chroma'ya vektÃ¶rÃ¼ gÃ¶nder
            n_results=top_k, 
            include=['documents', 'distances'] # Metni ve mesafeyi dÃ¶ndÃ¼r
        )

        # 3. BaÄŸlamÄ± formatla
        retrieved_chunks = results['documents'][0]
        
        print(f"\n[Geri AlÄ±nan BaÄŸlam ({top_k} adet)]:")
        for chunk in retrieved_chunks:
            print(f"- {chunk[:200]}...") 
            
        return "\n".join(retrieved_chunks)
        
    except Exception as e:
        print(f"Hata (retrieve_context): {e}")
        return f"BaÄŸlam alÄ±nÄ±rken bir hata oluÅŸtu: {e}"
from google.genai.types import GenerationConfig
def generate_response(query, context, history=None, temperature=0.5):
    """Geri alÄ±nan baÄŸlamÄ± ve sohbet geÃ§miÅŸini kullanarak Gemini'den yanÄ±t Ã¼retir (Ãœretim)."""
    
    # Sohbet geÃ§miÅŸini (history) formatlama
    history_str = ""
    if history:
        history_str += "\n\n--- SOHBET GEÃ‡MÄ°ÅÄ° ---\n"
        for message in history:
            role = "KullanÄ±cÄ±" if message["role"] == "user" else "Kahve Sever"
            history_str += f"{role}: {message['content']}\n"
        history_str += "-------------------------\n\n"
        
    try:
        # Prompt'a geÃ§miÅŸi dahil edin
        prompt = f"""
        Sen 'Kahve Sever' adÄ±nda bir uzmansÄ±n.
        GÃ¶rev: AÅŸaÄŸÄ±daki 'SOHBET GEÃ‡MÄ°ÅÄ°' ve 'BAÄLAM' bÃ¶lÃ¼mlerinde saÄŸlanan bilgilere dayanarak '{query}' sorusuna en uygun yanÄ±tÄ± verin.
        
        {history_str}
        
        AÅŸaÄŸÄ±daki 'BAÄLAM' bÃ¶lÃ¼mÃ¼nde saÄŸlanan bilgilere dayanarak '{query}' sorusuna yanÄ±t verin.
        EÄŸer BAÄLAM'da veya SOHBET GEÃ‡MÄ°ÅÄ°'nde yeterli bilgi yoksa, 'ÃœzgÃ¼nÃ¼m, bu kahve bilgisine sahip deÄŸilim.' diye cevap verin.

        BAÄLAM:
        {context} 

        SORU: {query}
        YANIT:
        """
        
        print(f"\n[YanÄ±t Ãœretiliyor - Temperature: {temperature}]")
        generation_model = genai.GenerativeModel('gemini-2.0-flash')
        
        # *** DEÄÄ°ÅÄ°KLÄ°K 1: Configuration nesnesi oluÅŸturuldu ***
        generation_config_dict = {
            "temperature": temperature
            # Ä°leride max_output_tokens, top_k gibi ayarlar eklemek isterseniz buraya ekleyin.
        }
        
        response = generation_model.generate_content(
            prompt, 
            generation_config=generation_config_dict # SÃ¶zlÃ¼ÄŸÃ¼ doÄŸrudan ilet
        )

        return response.text
    except Exception as e:
        print(f"Hata (generate_response): {e}")
        return f"YanÄ±t Ã¼retilirken bir hata oluÅŸtu: {e}"


def simple_rag_pipeline(query, history=None, temperature=0.5):
    """TÃ¼m RAG sÃ¼recini Ã§alÄ±ÅŸtÄ±rÄ±r ve SADECE nihai yanÄ±tÄ± dÃ¶ndÃ¼rÃ¼r."""
    # NOT: retrieve_context fonksiyonunun da temperature parametresini almasÄ± gerekir
    # ancak RAG aramasÄ± iÃ§in bu parametre gereksiz olduÄŸu iÃ§in ÅŸimdilik atlÄ±yoruz.
    # EÄŸer retrieve_context'in imzasÄ±na eklediyseniz, burada da alÄ±p iletmelisiniz.
    context = retrieve_context(query) 
    
    # Sohbet geÃ§miÅŸini (history) ve YENÄ° PARAMETRE temperature'Ä± generate_response'a iletin
    final_response = generate_response(
        query, 
        context, 
        history, 
        temperature=temperature # Streamlit'ten gelen deÄŸeri ilet
    ) 
    return final_response

# --- SICAKLIK KADEMELERÄ° TANIMI (Ã–nceki cevaptan) ---
TEMPERATURE_KADEMELERI = {
    0.00: "ğŸ”¬ Ciddi & Deterministik (Robot gibi kesin cevaplar, yaratÄ±cÄ±lÄ±k yok.)",
    0.25: "ğŸ“ DÃ¼ÅŸÃ¼k YaratÄ±cÄ±lÄ±k (Genellikle kesin, nadiren farklÄ± kelimeler kullanÄ±r.)",
    0.50: "ğŸ§  Dengeli & Standart (Ä°yi bir denge, Ã§oÄŸu soru iÃ§in Ã¶nerilir.)",
    0.75: "ğŸ¨ YÃ¼ksek YaratÄ±cÄ±lÄ±k (FarklÄ± kelimeler, benzersiz ifadeler kullanÄ±r. HalÃ¼sinasyon riski artar.)",
}

# Slider'Ä±n alabileceÄŸi deÄŸerler (sÃ¶zlÃ¼ÄŸÃ¼n anahtarlarÄ±)
ALLOWED_VALUES = list(TEMPERATURE_KADEMELERI.keys())
DEFAULT_VALUE = 0.50 


# --- 1. SÄ°DEBAR VE SICAKLIK AYARI ---

st.set_page_config(page_title="Coffeephil", page_icon="â˜•")
st.title("â˜• Coffeephil")
st.caption("Nitelikli kahve demleme yÃ¶ntemleri hakkÄ±nda her ÅŸeyi bana sorabilirsiniz.")

# Kenar Ã‡ubuÄŸuna SÄ±caklÄ±k AyarÄ± Ekleme
with st.sidebar:
    st.header("Model AyarlarÄ±")
    
    # SÄ±caklÄ±k Slider'Ä±
    selected_temperature = st.slider(
        label="SÄ±caklÄ±k (Temperature)",
        min_value=min(ALLOWED_VALUES),
        max_value=max(ALLOWED_VALUES),
        value=DEFAULT_VALUE,
        step=0.25, # Sadece kademeli deÄŸerleri seÃ§meyi garantiler.
        key="llm_temperature_slider", # Session state'de saklanmasÄ± iÃ§in bir anahtar
        help="Modelin cevap Ã¼retirken ne kadar yaratÄ±cÄ± ve rastgele olacaÄŸÄ±nÄ± belirler."
    )
    
    # SeÃ§ilen sÄ±caklÄ±ÄŸÄ±n aÃ§Ä±klamasÄ±nÄ± dinamik olarak gÃ¶steren kÄ±sÄ±m
    st.markdown("---")
    st.subheader("SeÃ§ilen Cevap TarzÄ±")
    description = TEMPERATURE_KADEMELERI.get(selected_temperature, "Bilinmeyen Ayar")
    
    st.markdown(
        f"**SÄ±caklÄ±k DeÄŸeri:** `{selected_temperature}`\n\n"
        f"**Tarz:** {description}"
    )


# --- 2. CHAT ARAYÃœZÃœ VE RAG MANTIÄI ---

# Sohbet geÃ§miÅŸini session_state'de sakla
if "messages" not in st.session_state:
    st.session_state.messages = []

# GeÃ§miÅŸ mesajlarÄ± gÃ¶ster
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ±dan yeni bir girdi al
if prompt := st.chat_input("Nitelikli kahve demleme teknikleri ile ilgili bir soru sorun..."):
    
    # 1. KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± sohbet geÃ§miÅŸine ekle ve ekranda gÃ¶ster
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Yeni eklendi: Son kullanÄ±cÄ± sorgusu HARÄ°Ã‡ tÃ¼m geÃ§miÅŸi al.
    history_for_rag = st.session_state.messages[:-1] 
    
    # 2. Chatbot'un yanÄ±tÄ±nÄ± al
    with st.spinner("Kahve Ã§ekirdeklerini inceliyorum..."):
        # Ã–NEMLÄ° DEÄÄ°ÅÄ°KLÄ°K: SÄ±caklÄ±k ayarÄ±nÄ± simple_rag_pipeline'a iletiyoruz.
        response = simple_rag_pipeline(
            query=prompt, 
            history=history_for_rag,
            temperature=selected_temperature # Streamlit'ten alÄ±nan deÄŸeri ilet
        )
    
    # 3. Chatbot'un yanÄ±tÄ±nÄ± ekranda gÃ¶ster
    with st.chat_message("assistant"):
        st.markdown(response)
    
    # 4. Chatbot'un yanÄ±tÄ±nÄ± sohbet geÃ§miÅŸine ekle
    st.session_state.messages.append({"role": "assistant", "content": response})